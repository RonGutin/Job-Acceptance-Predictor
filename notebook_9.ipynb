{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    " pip install joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.13' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/gutin/anaconda3/pkgs/python-3.9.13-h6244533_1/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Loading Data\n",
    "\n",
    "In this section, essential libraries for data manipulation, visualization, and modeling are imported. The dataset is also loaded for further analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,KFold, GridSearchCV, learning_curve\n",
    "from sklearn.ensemble import RandomForestClassifier , AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix\n",
    "df = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Visualization Functions\n",
    "\n",
    "This section includes custom functions for data visualization, outlier detection, handling missing values, normalization, and categorical feature processing. Additionally, it provides functions for plotting ROC curves, confusion matrices, and feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_histo(df, bins):\n",
    "    \"\"\"Plot histograms for each numerical feature in the DataFrame.\"\"\"\n",
    "    df.hist(bins=bins, figsize=(20, 15))\n",
    "    plt.show()\n",
    "\n",
    "def print_box(df):\n",
    "    \"\"\"Plot box plots for each numerical feature in the DataFrame.\"\"\"\n",
    "    df.plot(kind='box', subplots=True, layout=(4, 4), figsize=(20, 15))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_corr_mat(df):\n",
    "    \"\"\"Compute and plot the correlation matrix as a heatmap.\"\"\"\n",
    "    corr_matrix = df.corr()  # Compute the correlation matrix\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "    plt.show()\n",
    "\n",
    "def detect_outliers_z_score(df, columns,toprint=False):\n",
    "    \"\"\"Detect outliers using the Z-score method for the specified columns.\"\"\"\n",
    "    numeric_df = df[columns].select_dtypes(include=[np.number])\n",
    "    z_scores = np.abs(stats.zscore(numeric_df))\n",
    "    outliers = (z_scores > 3).any(axis=1)\n",
    "    if toprint: print(f\"Total amount of outliers in features : {columns} using Z-score method is: {outliers.sum()}\")\n",
    "    return outliers\n",
    "\n",
    "def detect_outliers_iqr(df, columns,toprint=False):\n",
    "    \"\"\"Detect outliers using the IQR method for the specified columns.\"\"\"\n",
    "    outlier_indices = set()\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = df[(df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))].index\n",
    "        outlier_indices.update(outliers)\n",
    "    if toprint: print(f\"Total amount of outliers in features : {columns} using IQR method is: {len(list(outlier_indices))}\")\n",
    "    return list(outlier_indices)\n",
    "\n",
    "def kick_outliers(df,toprint=False):\n",
    "    \"\"\"Detect and remove outliers using both Z-score and IQR methods.\"\"\"\n",
    "    normal_features = ['A', 'D']  # Assume normal distribution\n",
    "    non_normal_features = ['B', 'years_of_experience', 'prev_salary']  # Assume non normal distribution\n",
    "\n",
    "    # Detect outliers using Z-score and IQR methods\n",
    "    outliers_z_score = detect_outliers_z_score(df, normal_features,toprint)\n",
    "    outliers_iqr_indices = detect_outliers_iqr(df, non_normal_features,toprint)\n",
    "\n",
    "    # Combine indices of outliers and remove them from the DataFrame\n",
    "    outliers_combined = df.index[outliers_z_score | df.index.isin(outliers_iqr_indices)]\n",
    "    return df.drop(index=outliers_combined)\n",
    "\n",
    "def fix_null(df, df_test):\n",
    "    \"\"\"Handle missing values in the training and test DataFrames.\"\"\"\n",
    "    # Fill 'worked_in_the_past' based on 'years_of_experience'\n",
    "    df.loc[(df['years_of_experience'] > 0) & (df['worked_in_the_past'].isna()), 'worked_in_the_past'] = 'T'\n",
    "    df.loc[(df['years_of_experience'] == 0) & (df['worked_in_the_past'].isna()), 'worked_in_the_past'] = 'F'\n",
    "    median_years_of_experience = df['years_of_experience'].median()\n",
    "    df.loc[(df['worked_in_the_past'] == 'T') & (df['years_of_experience'].isna()), 'years_of_experience'] = median_years_of_experience\n",
    "    df.loc[(df['worked_in_the_past'] == 'F') & (df['years_of_experience'].isna()), 'years_of_experience'] = 0\n",
    "    condition = df['worked_in_the_past'].isna() & df['years_of_experience'].isna()\n",
    "    df.loc[condition, 'years_of_experience'] = 0\n",
    "    df.loc[condition, 'worked_in_the_past'] = 'F'\n",
    "\n",
    "    # Fill missing categorical values with 'Other' or mode\n",
    "    df[['sex', 'education']] = df[['sex', 'education']].fillna({'sex': 'Other', 'education': 'other'})\n",
    "    for col in ['is_dev', 'disability', 'mental_issues', 'C', 'country', 'age_group']:\n",
    "        mode_value = df[col].mode()[0]\n",
    "        df[col] = df[col].fillna(mode_value)\n",
    "\n",
    "    # Fill missing numerical values with the median\n",
    "    num_features = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in num_features:\n",
    "        if col not in ['ID', 'years_of_experience', 'label']:\n",
    "            median = df[col].median()\n",
    "            df[col] = df[col].fillna(median)\n",
    "            df_test[col] = df_test[col].fillna(median)\n",
    "\n",
    "    # Handle missing values in the test DataFrame similarly\n",
    "    df_test.loc[(df['years_of_experience'] > 0) & (df_test['worked_in_the_past'].isna()), 'worked_in_the_past'] = 'T'\n",
    "    df_test.loc[(df['years_of_experience'] == 0) & (df_test['worked_in_the_past'].isna()), 'worked_in_the_past'] = 'F'\n",
    "    df_test.loc[(df_test['worked_in_the_past'] == 'T') & (df_test['years_of_experience'].isna()), 'years_of_experience'] = median_years_of_experience\n",
    "    df_test.loc[(df_test['worked_in_the_past'] == 'F') & (df_test['years_of_experience'].isna()), 'years_of_experience'] = 0\n",
    "    condition = df_test['worked_in_the_past'].isna() & df_test['years_of_experience'].isna()\n",
    "    df_test.loc[condition, 'years_of_experience'] = 0\n",
    "    df_test.loc[condition, 'worked_in_the_past'] = 'F'\n",
    "\n",
    "    df_test[['sex', 'education']] = df_test[['sex', 'education']].fillna({'sex': 'Other', 'education': 'other'})\n",
    "    for col in ['is_dev', 'disability', 'mental_issues', 'C', 'country', 'age_group']:\n",
    "        mode_value = df[col].mode()[0]\n",
    "        df_test[col] = df_test[col].fillna(mode_value)\n",
    "\n",
    "    # Drop the 'label' column from the test DataFrame if it exists\n",
    "    if 'label' in df_test.columns:\n",
    "        df_test.drop(columns=['label'], inplace=True)\n",
    "        \n",
    "    return df, df_test\n",
    "\n",
    "def norma_numeric(df, df_test):\n",
    "    \"\"\"Normalize numerical features using Min-Max scaling.\"\"\"\n",
    "    numerical_features = ['years_of_experience', 'A', 'B', 'D', 'prev_salary']\n",
    "\n",
    "    # Apply Min-Max scaling to the numerical features\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    df[numerical_features] = min_max_scaler.fit_transform(df[numerical_features])\n",
    "    df_test[numerical_features] = min_max_scaler.transform(df_test[numerical_features])\n",
    "\n",
    "    return df, df_test\n",
    "\n",
    "def handle_catg(df, df_test):\n",
    "    \"\"\"Process and encode categorical features in the training and test DataFrames.\"\"\"\n",
    "    # Encode binary categorical features\n",
    "    df['is_dev'] = df['is_dev'].apply(lambda x: 1 if x == 'developer' else 0)\n",
    "    df['disability'] = df['disability'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "    \n",
    "    # Map education levels to numeric values\n",
    "    education_mapping = {\n",
    "        'BA/BSc': 2,\n",
    "        'MA/MSc': 3,\n",
    "        'Phd': 4,\n",
    "        'High school': 1,\n",
    "        'other': 0\n",
    "    }\n",
    "    df['education'] = df['education'].replace(education_mapping)\n",
    "    df['age_group'] = df['age_group'].apply(lambda x: 1 if x == 'young' else 0)\n",
    "    df['worked_in_the_past'] = df['worked_in_the_past'].apply(lambda x: 1 if x == 'T' else 0)\n",
    "    df['mental_issues'] = df['mental_issues'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "    # One-hot encode the 'sex' column\n",
    "    df = pd.get_dummies(df, columns=['sex'], prefix='sex')\n",
    "\n",
    "    # Process 'stack_experience' feature\n",
    "    df['stack_experience'] = df['stack_experience'].str.lower()\n",
    "    skill_count = {}\n",
    "    for skills in df['stack_experience']:\n",
    "        if pd.isnull(skills):\n",
    "            continue\n",
    "        skills_list = skills.split(';')\n",
    "        for skill in skills_list:\n",
    "            skill = skill.strip().lower()\n",
    "            if skill in skill_count:\n",
    "                skill_count[skill] += 1\n",
    "            else:\n",
    "                skill_count[skill] = 1\n",
    "\n",
    "    skill_count_df = pd.DataFrame(list(skill_count.items()), columns=['Skill', 'Count'])\n",
    "    threshold = 500\n",
    "    filtered_skills = skill_count_df[skill_count_df['Count'] >= threshold]['Skill'].tolist()\n",
    "\n",
    "    for skill in filtered_skills:\n",
    "        df[skill] = 0\n",
    "    for idx, skills in df['stack_experience'].iteritems():\n",
    "        if pd.isnull(skills):\n",
    "            continue\n",
    "        skills_list = skills.split(';')\n",
    "        for skill in skills_list:\n",
    "            skill = skill.strip().lower()\n",
    "            if skill in filtered_skills:\n",
    "                df.at[idx, skill] = 1\n",
    "    df.drop(columns=['stack_experience'], inplace=True)\n",
    "\n",
    "    # Process the test DataFrame similarly\n",
    "    for skill in filtered_skills:\n",
    "        df_test[skill] = 0\n",
    "    for idx, skills in df_test['stack_experience'].iteritems():\n",
    "        if pd.isnull(skills):\n",
    "            continue\n",
    "        skills_list = skills.split(';')\n",
    "        for skill in skills_list:\n",
    "            skill = skill.strip().lower()\n",
    "            if skill in filtered_skills:\n",
    "                df_test.at[idx, skill] = 1\n",
    "    df_test['is_dev'] = df_test['is_dev'].apply(lambda x: 1 if x == 'developer' else 0)\n",
    "    df_test['disability'] = df_test['disability'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "    df_test['education'] = df_test['education'].replace(education_mapping)\n",
    "    df_test['age_group'] = df_test['age_group'].apply(lambda x: 1 if x == 'young' else 0)\n",
    "    df_test['worked_in_the_past'] = df_test['worked_in_the_past'].apply(lambda x: 1 if x == 'T' else 0)\n",
    "    df_test = pd.get_dummies(df_test, columns=['sex'], prefix='sex')\n",
    "    df_test['mental_issues'] = df_test['mental_issues'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "    df_test.drop(columns=['stack_experience'], inplace=True)\n",
    "\n",
    "    return df, df_test\n",
    "\n",
    "def plot_roc_curve(fpr_train, tpr_train, fpr_test, tpr_test, roc_auc_train, roc_auc_test, model):\n",
    "    \"\"\"Plot the ROC curve for training and test sets.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(fpr_train, tpr_train, color='blue', lw=2, label='Train ROC curve (area = %0.2f)' % roc_auc_train)\n",
    "    plt.plot(fpr_test, tpr_test, color='red', lw=2, label='Test ROC curve (area = %0.2f)' % roc_auc_test)\n",
    "    plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Receiver Operating Characteristic (ROC) - {model}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    \"\"\"Plot the confusion matrix.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(cm.shape[0])\n",
    "    plt.xticks(tick_marks, ['0', '1'])\n",
    "    plt.yticks(tick_marks, ['0', '1'])\n",
    "    threshold = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > threshold else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "def importance_feat(model):\n",
    "    \"\"\"Plot the feature importance for a given model.\"\"\"\n",
    "    feature_importances = model.feature_importances_\n",
    "    features = X_train.columns\n",
    "\n",
    "    # Create a DataFrame for the feature importances\n",
    "    importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "\n",
    "    # Sort the DataFrame by importance\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "    print(importance_df.head(40))\n",
    "\n",
    "    # Plot the feature importances\n",
    "    plt.figure(figsize=(150, 50))\n",
    "    plt.bar(importance_df['Feature'], importance_df['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Feature Importance in Random Forest Classifier')\n",
    "    plt.gca()\n",
    "    plt.show()\n",
    "    print(importance_df.iloc[60:]['Feature'].tolist())\n",
    "\n",
    "def plot_roc_curve_kfold(fpr_list, tpr_list, auc_list, model_name, n_folds):\n",
    "    \"\"\"Plot the ROC curve for each fold in K-fold cross-validation.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in range(n_folds):\n",
    "        plt.plot(fpr_list[i], tpr_list[i], lw=2, \n",
    "                 label=f'Fold {i+1} ROC curve (area = {auc_list[i]:0.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Receiver Operating Characteristic (ROC) - {model_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Initial Data Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data head:\\n\")\n",
    "print(df.head()) # Initial impressions of the data, see column names, and understand the general format.\n",
    "print(\"Info:\")\n",
    "print(df.info()) # Number of non-null entries, data types of each column, memory usage.\n",
    "print(\"\\nData description\\n\")\n",
    "print(df.describe(include='all')) # Count, mean, standard deviation, min, max, and percentiles for numerical features, frequency counts for categorical features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we explore the distribution of numerical data by examining the skewness, spread, and presence of outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learn: Skewness, spread, and the presence of outliers in numerical data.\n",
    "print_histo(df.drop(columns=['ID','label']),50)\n",
    "print_box(df.drop(columns=['ID','label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conculsion: \n",
    "We assume that A and D are normally distributed while B, prev_salary and years_of_expereince aren't.\n",
    "We can see in the box plots that outliers exist in all features.This indicates that there are values in each feature that significantly deviate from the median.\n",
    "'D' has a very narrow IQR, indicating that the majority of data points are closely packed within a small range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Data\n",
    "In this section, we examine the distribution and potential imbalance in categorical data. We will identify common and rare categories for each categorical feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution and imbalance in categorical data, common and rare categories.\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    print(df[column].value_counts())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "The dataset reveals a diverse range of categorical and numerical features, with some significant imbalances. The majority of individuals have worked in the past, are younger, and do not report disabilities. There is a higher proportion of developers compared to non-developers. Numerical features like `years_of_experience` and `prev_salary` vary widely, indicating diverse backgrounds. Some missing values in `education` and `country` will need to be addressed during data cleaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique Values\n",
    "In this section, we examine the number of unique values in each column to identify potential categorical columns and check for high cardinality features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique values in each column\n",
    "print(df.nunique())\n",
    "# Detect potential categorical columns, check for high cardinality features.\n",
    "#stack experience - large variety of technology skills\n",
    "# A (53327 unique values), B (51 unique values), C (7 unique values), D (52843 unique values): These columns might represent some numerical measures or derived features. The high number of unique values in A and D suggests continuous or nearly continuous data, while B and C might have more discrete values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickly identify highly correlated features, potential multicollinearity issues.\n",
    "print_corr_mat(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "The correlation heatmap reveals a strong positive correlation (0.9) between 'years_of_experience' and 'B', indicating potential multicollinearity. The feature 'D' shows a moderate positive correlation with the label (0.41), suggesting its importance in prediction. Most other features have weak correlations with each other and the target, implying they offer independent information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pair Plot\n",
    "Visualization for the high correlation between B & years_of_experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot for a subset of features\n",
    "sns.pairplot(df[['years_of_experience' ,'B']])\n",
    "plt.show()\n",
    "#Scatter plots and histograms for feature pairs, understand potential interactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that A and D are normally distributed while B, prev_salary and years_of_expereince doesn't. Therefore, we will use Z-Score to identify outliers for A and D and IQR for B, prev_salary and years_of_expereince"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kick_outliers(df,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems some features might not be normalized like: A, B, C, D,years_of_experience, and prev_salary.\n",
    "Normalization is particularly important in machine learning because:\n",
    "\n",
    "Feature Importance: Normalization can make it easier to interpret feature importance when comparing the influence of features on the model.\n",
    "\n",
    "Data Consistency: Preprocessing the data with normalization, will ensure consistency in data preparation\n",
    "\n",
    "Improved Performance: For some models, normalization can improve performance and lead to better results because the model doesn't have to deal with large variations in feature scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the data using Min-Max Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df,df_test = norma_numeric(df,df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = df.isnull().sum()\n",
    "print(missing_data)\n",
    "\n",
    "df,df_test = fix_null(df,df_test)\n",
    "print(\"\\nAfter filling:\\n\")\n",
    "missing_data = df.isnull().sum()\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Catagorial Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df,df_test = handle_catg(df,df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 16 features (excluding the label), which is manageable for most algorithms. While high dimensionality isn't a major concern here, further steps like assessing feature importance and applying PCA could optimize the model.\n",
    "\n",
    "High Dimensionality Issues:\n",
    "\n",
    "Curse of Dimensionality: Data points become sparse, making pattern detection difficult.\n",
    "Overfitting: More features can cause models to fit noise rather than true patterns, leading to poor generalization.\n",
    "Increased Computational Cost: More features require more processing resources.\n",
    "Data Redundancy: Irrelevant or redundant features can add noise and complexity.\n",
    "Interpretability: Models with many features are harder to understand and trust.\n",
    "\n",
    "Evaluating Dimensionality:\n",
    "\n",
    "Model Performance: Poor validation performance compared to training could indicate overfitting.\n",
    "Feature Importance: Identifying features with minimal impact can suggest redundancy.\n",
    "PCA: Determines if a few components explain most of the variance, indicating if the feature space is too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop specified columns from the DataFrame\n",
    "feat = df.drop(columns=['label', 'country', 'ID','C'])\n",
    "print(feat.shape)\n",
    "\n",
    "# Standardize the features before PCA\n",
    "scaler = StandardScaler()\n",
    "X_pca = scaler.fit_transform(feat)\n",
    "\n",
    "# Apply PCA to retain 99% of the variance\n",
    "pca_sol = PCA(0.99)\n",
    "pca_sol.fit(X_pca)\n",
    "\n",
    "# Get the number of components and the eigenvalues\n",
    "components = pca_sol.components_\n",
    "print(len(components))\n",
    "\n",
    "# Get the eigenvalues\n",
    "eigenvalues = pca_sol.explained_variance_\n",
    "total_explained_variance = np.sum(pca_sol.explained_variance_ratio_)\n",
    "print(f\"Total explained variance: {total_explained_variance:.2f}\")\n",
    "\n",
    "# Transform the data using PCA\n",
    "X_reduced = pd.DataFrame(pca_sol.transform(X_pca))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the same columns as in the training set\n",
    "feat_test = df_test.drop(columns=['country', 'ID', 'C'])\n",
    "train_columns = feat.columns\n",
    "feat_test = df_test.reindex(columns=train_columns)\n",
    "# Scale the test data using the scaler fitted on the training data\n",
    "X_test_scaled = scaler.transform(feat_test)\n",
    "\n",
    "# Apply the PCA transformation using the PCA fitted on the training data\n",
    "X_test_pca = pd.DataFrame(pca_sol.transform(X_test_scaled), columns=[f'PC{i+1}' for i in range(len(components))])\n",
    "\n",
    "# X_test_pca contains the PCA-transformed features of df_test, ensuring consistency with the PCA fitted on the training set (avoiding data leakage).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data to train,validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.drop(columns=['label','ID','C','B','country','mental_issues','sex_Male','sex_Female','sex_Other','disability','age_group','worked_in_the_past'])\n",
    "\n",
    "y_train = df['label']\n",
    "train_and_validation_data,test_data,train_and_validation_labels,test_labels = train_test_split(X_train,y_train,\n",
    "                                                        test_size = 0.25,\n",
    "                                                        random_state= 42, \n",
    "                                                        shuffle=True,\n",
    "                                                        stratify = y_train)\n",
    "\n",
    "# stratify ensures that the training and test sets have the same proportion of each class label as the original dataset\n",
    "# Splitting the training data further into train and validation sets for hyperparameter tuning and model evaluation\n",
    "train_data,validation_data ,train_labels,validation_labels = train_test_split(train_and_validation_data,train_and_validation_labels,\n",
    "                                                        test_size = 0.2,\n",
    "                                                        random_state= 42, \n",
    "                                                        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3+4: Model Test Run and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation for optimal C (hyperparameter for logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powers = range(-10, 0)\n",
    "Cs = [10**p for p in powers]\n",
    "c_scores = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
    "\n",
    "for c in Cs:\n",
    "    reg_log = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", random_state=42, C=c)\n",
    "    \n",
    "    # Cross-validation score for the current value of C\n",
    "    scores = cross_val_score(reg_log, train_and_validation_data, train_and_validation_labels, cv=kf, scoring='roc_auc')\n",
    "    c_scores.append(np.mean(scores))\n",
    "\n",
    "plt.plot(powers, c_scores)\n",
    "plt.show()\n",
    "\n",
    "C_opt_lr = Cs[c_scores.index(max(c_scores))]\n",
    "lambda_opt_lr = 1 / C_opt_lr\n",
    "\n",
    "print(\"The optimal lambda is:\", lambda_opt_lr, \"\\n(Best C:\", C_opt_lr, \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model: Training, Evaluation, and Performance Metrics\n",
    "\n",
    "In this section, we initialize, train and evaluate a Logistic Regression model using the provided training and testing data. The evaluation includes calculating the ROC AUC, plotting ROC curves, and confusion matrices, as well as computing the accuracy for both training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize and fit Logistic Regression model\n",
    "log_reg = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", C=C_opt_lr)\n",
    "log_reg.fit(train_and_validation_data, train_and_validation_labels)\n",
    "\n",
    "# Predictions and Probabilities\n",
    "train_probs = log_reg.predict_proba(train_and_validation_data)[:, 1]\n",
    "test_probs = log_reg.predict_proba(test_data)[:, 1]\n",
    "train_prediction = log_reg.predict(train_and_validation_data)\n",
    "test_prediction = log_reg.predict(test_data)\n",
    "\n",
    "# ROC AUC for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(train_and_validation_labels, train_probs)\n",
    "roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "# ROC AUC for testing data\n",
    "fpr_test, tpr_test, _ = roc_curve(test_labels, test_probs)\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plot_roc_curve(fpr_train, tpr_train, fpr_test, tpr_test, roc_auc_train, roc_auc_test,\"Logistic Regression\")\n",
    "\n",
    "# Confusion Matrix for Train and Test Data\n",
    "conf_matrix_train = confusion_matrix(train_and_validation_labels, train_prediction)\n",
    "conf_matrix_test = confusion_matrix(test_labels, test_prediction)\n",
    "\n",
    "# Plot Confusion Matrices\n",
    "plot_confusion_matrix(conf_matrix_train, 'Confusion Matrix - Train Data')\n",
    "plot_confusion_matrix(conf_matrix_test, 'Confusion Matrix - Test Data')\n",
    "\n",
    "# Accuracy Calculations\n",
    "accur_train = np.trace(conf_matrix_train) / np.sum(conf_matrix_train)\n",
    "accur_test = np.trace(conf_matrix_test) / np.sum(conf_matrix_test)\n",
    "print(f'Accuracy on train data: {accur_train:.4f}')\n",
    "print(f'Accuracy on test data: {accur_test:.4f}')\n",
    "print(\"Test confusion Matrix: \\n\", conf_matrix_test)\n",
    "tn, fp, fn, tp = conf_matrix_test.ravel()\n",
    "print(\"Test Accuracy: \\n\", (tp + tn) / conf_matrix_test.sum())\n",
    "\n",
    "# Print AUC values\n",
    "print(f'Training AUC: {roc_auc_train:.4f}')\n",
    "print(f'Test AUC: {roc_auc_test:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning for K-Nearest Neighbors (KNN): Cross-Validation with Varying Number of Neighbors\n",
    "\n",
    "In this section, we perform hyperparameter tuning for the K-Nearest Neighbors (KNN) classifier by varying the number of neighbors (`k`). The goal is to identify the optimal `k` value that maximizes the model's performance as measured by the AUC-ROC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "k_folds = 3\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=1)\n",
    "\n",
    "# Range of k values to test\n",
    "k_range = list(range(1, 31)) + list(range(35, 100, 5)) + list(range(100, 225, 10))\n",
    "\n",
    "# Function to evaluate a single k value using AUC-ROC\n",
    "def evaluate_k(k):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    auc_scores = cross_val_score(knn, X_train, y_train, cv=kf, scoring=\"roc_auc\")\n",
    "    return auc_scores.mean()\n",
    "\n",
    "# Perform parallel cross-validation for each k\n",
    "k_scores = Parallel(n_jobs=-1)(delayed(evaluate_k)(k) for k in k_range)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(k_range, k_scores, marker='o')\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Cross-Validated AUC-ROC')\n",
    "plt.title('KNN Varying number of neighbors')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find the optimal k\n",
    "optimal_k = k_range[np.argmax(k_scores)]\n",
    "print(f'The optimal number of neighbors is {optimal_k}')\n",
    "######################################\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors (KNN) Model: Training, Evaluation, and Performance Metrics\n",
    "\n",
    "In this section, we train and evaluate a K-Nearest Neighbors (KNN) classifier using the optimal number of neighbors (`k`) determined from previous cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimal_k=17\n",
    "knn_model = KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "knn_model.fit(train_and_validation_data, train_and_validation_labels)\n",
    "\n",
    "# Predictions\n",
    "train_prediction = knn_model.predict(train_and_validation_data)\n",
    "test_prediction = knn_model.predict(test_data)\n",
    "\n",
    "# Probabilities for ROC AUC\n",
    "train_probs = knn_model.predict_proba(train_and_validation_data)[:, 1]\n",
    "test_probs = knn_model.predict_proba(test_data)[:, 1]\n",
    "\n",
    "# ROC AUC for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(train_and_validation_labels, train_probs)\n",
    "roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "# ROC AUC for testing data\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(test_labels, test_probs)\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "plot_roc_curve(fpr_train, tpr_train, fpr_test, tpr_test, roc_auc_train, roc_auc_test,\"KNN\")\n",
    "\n",
    "\n",
    "# Accuracy on Train Data\n",
    "accur_train = np.trace(conf_matrix_train) / np.sum(conf_matrix_train)\n",
    "print(f'Accuracy on train data: {accur_train:.4f}')\n",
    "\n",
    "# Confusion Matrix for Train and Test Data\n",
    "conf_matrix_train = confusion_matrix(train_and_validation_labels, train_prediction)\n",
    "conf_matrix_test = confusion_matrix(test_labels, test_prediction)\n",
    "\n",
    "# Plot Confusion Matrices\n",
    "plot_confusion_matrix(conf_matrix_train, 'Confusion Matrix - Train Data')\n",
    "plot_confusion_matrix(conf_matrix_test, 'Confusion Matrix - Test Data')\n",
    "\n",
    "\n",
    "# Accuracy on Test Data\n",
    "accur_test = np.trace(conf_matrix_test) / np.sum(conf_matrix_test)\n",
    "print(f'Accuracy on test data: {accur_test:.4f}')\n",
    "print(\"Test confusion Matrix: \\n\", conf_matrix_test)\n",
    "tn, fp, fn, tp = conf_matrix_test.ravel()\n",
    "print(\"Test Accuracy: \\n\", (tp + tn) / conf_matrix_test.sum())\n",
    "\n",
    "# Print AUC values\n",
    "print(f'Training AUC: {roc_auc_train:.4f}')\n",
    "print(f'Test AUC: {roc_auc_test:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model: Hyperparameter Tuning, Training, and Evaluation\n",
    "\n",
    "In this section, we perform hyperparameter tuning for a Random Forest model using `GridSearchCV`, followed by training and evaluating the model using the best-found parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'criterion':['gini', 'entropy'],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='roc_auc', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(train_data, train_labels)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f'Best Params: {best_params} \\n')\n",
    "print(f'Validation AUC score for best params: {best_score}')\n",
    "# Initialize Random Forest model with some regularization\n",
    "'''\n",
    "# Best parameters\n",
    "best_params = {\n",
    "    'criterion': 'entropy',\n",
    "    'max_depth': 15,\n",
    "    'max_features': 'sqrt',\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_samples_split': 5,\n",
    "    'n_estimators': 150\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier with the best parameters\n",
    "rf_model = RandomForestClassifier(\n",
    "    criterion=best_params['criterion'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    max_features=best_params['max_features'],\n",
    "    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    random_state=42 \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(train_and_validation_data, train_and_validation_labels)\n",
    "\n",
    "# Predictions\n",
    "train_prediction = rf_model.predict(train_and_validation_data)\n",
    "test_prediction = rf_model.predict(test_data)\n",
    "\n",
    "# Probabilities for ROC AUC\n",
    "train_probs = rf_model.predict_proba(train_and_validation_data)[:, 1]\n",
    "test_probs = rf_model.predict_proba(test_data)[:, 1]\n",
    "\n",
    "# ROC AUC for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(train_and_validation_labels, train_probs)\n",
    "roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "# ROC AUC for testing data\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(test_labels, test_probs)\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "#################\n",
    "\n",
    "\n",
    "# Plot ROC Curve\n",
    "plot_roc_curve(fpr_train, tpr_train, fpr_test, tpr_test, roc_auc_train, roc_auc_test,\"Random Forest\")\n",
    "\n",
    "# Confusion Matrix for Train and Test Data\n",
    "conf_matrix_train = confusion_matrix(train_and_validation_labels, train_prediction)\n",
    "conf_matrix_test = confusion_matrix(test_labels, test_prediction)\n",
    "\n",
    "# Plot Confusion Matrices\n",
    "plot_confusion_matrix(conf_matrix_train, 'Confusion Matrix - Train Data')\n",
    "plot_confusion_matrix(conf_matrix_test, 'Confusion Matrix - Test Data')\n",
    "\n",
    "\n",
    "\n",
    "print(f'Training AUC: {roc_auc_train:.4f}')\n",
    "print(f'Test AUC: {roc_auc_test:.4f}')\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "#importance_feat(rf_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost Model: Hyperparameter Setup, Training, Evaluation, and Validation\n",
    "\n",
    "In this section, we define the best hyperparameters for an AdaBoost model, initialize the model, and evaluate its performance. Additionally, we validate the model using cross-validation and learning curves to check for overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Define the parameter grid for both AdaBoost and the base DecisionTreeClassifier\n",
    "param_grid = {\n",
    "    'base_estimator__max_depth': [1, 2, 3],\n",
    "    'base_estimator__min_samples_split': [3, 5, 10],\n",
    "    'base_estimator__min_samples_leaf': [1, 2, 3],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the base estimator\n",
    "base_estimator = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Initialize the AdaBoost classifier\n",
    "ada_clf = AdaBoostClassifier(base_estimator=base_estimator, random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV with AdaBoost and the parameter grid\n",
    "grid_search = GridSearchCV(estimator=ada_clf, param_grid=param_grid, scoring='roc_auc', cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the GridSearch to the data\n",
    "grid_search.fit(train_and_validation_data, train_and_validation_labels)\n",
    "\n",
    "# Extract the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f'Best Parameters: {best_params}')\n",
    "'''\n",
    "\n",
    "best_params = {\n",
    "    'base_estimator__ccp_alpha': 0.0,\n",
    "    'base_estimator__class_weight': None,\n",
    "    'base_estimator__criterion': 'gini',\n",
    "    'base_estimator__max_depth': 2,\n",
    "    'base_estimator__max_features': None,\n",
    "    'base_estimator__max_leaf_nodes': None,\n",
    "    'base_estimator__min_impurity_decrease': 0.0,\n",
    "    'base_estimator__min_samples_leaf': 3,\n",
    "    'base_estimator__min_samples_split': 2,\n",
    "    'base_estimator__min_weight_fraction_leaf': 0.0,\n",
    "    'base_estimator__random_state': 42,\n",
    "    'base_estimator__splitter': 'best',\n",
    "    'n_estimators': 200,  \n",
    "    'random_state': 42  # This is typically kept for reproducibility\n",
    "}\n",
    "\n",
    "# Initialize the base estimator\n",
    "base_estimator = DecisionTreeClassifier(\n",
    "    max_depth=best_params['base_estimator__max_depth'],\n",
    "    min_samples_leaf=best_params['base_estimator__min_samples_leaf'],\n",
    "    min_samples_split=best_params['base_estimator__min_samples_split'],\n",
    "    random_state=best_params['base_estimator__random_state']\n",
    ")\n",
    "\n",
    "# Initialize AdaBoost classifier with optimized parameters obtained from GridSearchCV\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    base_estimator=base_estimator,\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    random_state=best_params['random_state']\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model (assuming train_and_validation_data and train_and_validation_labels are defined)\n",
    "ada_clf.fit(train_and_validation_data, train_and_validation_labels)\n",
    "\n",
    "# Predictions\n",
    "train_prediction = ada_clf.predict(train_and_validation_data)\n",
    "train_probs = ada_clf.predict_proba(train_and_validation_data)[:, 1]\n",
    "\n",
    "# Make predictions on the test set (assuming test_data and test_labels are defined)\n",
    "test_pred = ada_clf.predict(test_data)\n",
    "test_prob = ada_clf.predict_proba(test_data)[:, 1]\n",
    "\n",
    "# Confusion Matrices\n",
    "conf_matrix_train = confusion_matrix(train_and_validation_labels, train_prediction)\n",
    "conf_matrix_test = confusion_matrix(test_labels, test_pred)\n",
    "\n",
    "# Plot Confusion Matrices\n",
    "\n",
    "plot_confusion_matrix(conf_matrix_train, 'Confusion Matrix - Train Data')\n",
    "plot_confusion_matrix(conf_matrix_test, 'Confusion Matrix - Test Data')\n",
    "\n",
    "# ROC AUC for test data\n",
    "fpr_test, tpr_test, _ = roc_curve(test_labels, test_prob)\n",
    "roc_auc_test = roc_auc_score(test_labels, test_prob)\n",
    "\n",
    "# ROC AUC for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(train_and_validation_labels, train_probs)\n",
    "roc_auc_train = roc_auc_score(train_and_validation_labels, train_probs)\n",
    "\n",
    "plot_roc_curve(fpr_train, tpr_train, fpr_test, tpr_test, roc_auc_train, roc_auc_test,\"AdaBoost\")\n",
    "\n",
    "print(f'Training AUC: {roc_auc_train:.4f}')\n",
    "print(f'Test AUC: {roc_auc_test:.4f}')\n",
    "\n",
    "# Define the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "\n",
    "# Lists to store fpr, tpr, and auc values for each fold\n",
    "fpr_list = []\n",
    "tpr_list = []\n",
    "auc_list = []\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation and plot ROC curves for each fold\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    ada_clf.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predict probabilities for the validation fold\n",
    "    val_probs = ada_clf.predict_proba(X_val_fold)[:, 1]\n",
    "    \n",
    "    # Calculate ROC curve and AUC for the validation fold\n",
    "    fpr, tpr, _ = roc_curve(y_val_fold, val_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    fpr_list.append(fpr)\n",
    "    tpr_list.append(tpr)\n",
    "    auc_list.append(roc_auc)\n",
    "\n",
    "\n",
    "# Plot ROC curve for each fold\n",
    "plot_roc_curve_kfold(fpr_list, tpr_list, auc_list, \"AdaBoost\", n_folds)\n",
    "\n",
    "# Generate learning curves\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    ada_clf, X_train, y_train, cv=5, scoring='roc_auc', n_jobs=-1, \n",
    "    train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "\n",
    "# Calculate the mean and standard deviation of the scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plotting the learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training score')\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color='g', label='Cross-validation score')\n",
    "\n",
    "# Adding fill between for standard deviation\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1, color='r')\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color='g')\n",
    "\n",
    "plt.xlabel('Training examples')\n",
    "plt.ylabel('ROC AUC Score')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Check for overfitting\n",
    "if train_scores_mean[-1] - test_scores_mean[-1] > 0.1:  # Adjust the threshold as necessary\n",
    "    print(\"The model is likely overfitting. The training score is much higher than the cross-validation score.\")\n",
    "else:\n",
    "    print(\"The model does not appear to be overfitting.\")\n",
    "\n",
    "# Refit model on entire training data\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and ROC AUC for full training set\n",
    "X_probs = ada_clf.predict_proba(X_train)[:, 1]\n",
    "fpr_X, tpr_X, _ = roc_curve(y_train, X_probs)\n",
    "roc_auc_full_train = roc_auc_score(y_train, X_probs)\n",
    "\n",
    "# Predictions\n",
    "X_prediction = ada_clf.predict(X_train)\n",
    "\n",
    "# Confusion Matrices\n",
    "conf_matrix_X = confusion_matrix(y_train, X_prediction)\n",
    "\n",
    "# Plot Confusion Matrices\n",
    "plot_confusion_matrix(conf_matrix_X, 'Confusion Matrix - All Data')\n",
    "\n",
    "# ROC AUC for test data\n",
    "fpr_X, tpr_X, _ = roc_curve(y_train, X_probs)\n",
    "\n",
    "print(f'X AUC: {roc_auc_full_train:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Prediction\n",
    "## End-to-End Machine Learning Pipeline for AdaBoost Model\n",
    "*The libraries and the function cell must be loaded before*\n",
    "\n",
    "This section implements an end-to-end machine learning pipeline that includes data preprocessing, model training, and prediction generation using an AdaBoost classifier with a `DecisionTreeClassifier` as the base estimator. The pipeline processes both training and test datasets, trains the model, and produces prediction probabilities for the test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline():\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    df_test=pd.read_csv(\"test.csv\")\n",
    "    df = kick_outliers(df)\n",
    "    df,df_test = norma_numeric(df,df_test)    \n",
    "    df,df_test = fix_null(df,df_test)\n",
    "    df,df_test = handle_catg(df,df_test)\n",
    "    X_train = df.drop(columns=['label','ID','C','B','country','mental_issues','sex_Male','sex_Female','sex_Other','disability','age_group','worked_in_the_past'])\n",
    "    y_train = df['label']\n",
    "\n",
    "\n",
    "    best_params = {\n",
    "        'base_estimator__ccp_alpha': 0.0,\n",
    "        'base_estimator__class_weight': None,\n",
    "        'base_estimator__criterion': 'gini',\n",
    "        'base_estimator__max_depth': 2,\n",
    "        'base_estimator__max_features': None,\n",
    "        'base_estimator__max_leaf_nodes': None,\n",
    "        'base_estimator__min_impurity_decrease': 0.0,\n",
    "        'base_estimator__min_samples_leaf': 3,\n",
    "        'base_estimator__min_samples_split': 2,\n",
    "        'base_estimator__min_weight_fraction_leaf': 0.0,\n",
    "        'base_estimator__random_state': 42,\n",
    "        'base_estimator__splitter': 'best',\n",
    "        'n_estimators': 200,  \n",
    "        'random_state': 42  \n",
    "    }\n",
    "\n",
    "    # Initialize the base estimator\n",
    "    base_estimator = DecisionTreeClassifier(\n",
    "        max_depth=best_params['base_estimator__max_depth'],\n",
    "        min_samples_leaf=best_params['base_estimator__min_samples_leaf'],\n",
    "        min_samples_split=best_params['base_estimator__min_samples_split'],\n",
    "        random_state=best_params['base_estimator__random_state']\n",
    "    )\n",
    "\n",
    "    # Initialize AdaBoost classifier with simplified parameters\n",
    "    ada_clf = AdaBoostClassifier(\n",
    "        base_estimator=base_estimator,\n",
    "        n_estimators=best_params['n_estimators'],\n",
    "        random_state=best_params['random_state']\n",
    "    )\n",
    "\n",
    "    # Fit model on entire training data\n",
    "    ada_clf.fit(X_train, y_train)\n",
    "\n",
    "    id_column = (pd.read_csv(\"test.csv\"))['ID']\n",
    "    df_test.drop(columns=['ID','C','B','country','mental_issues','sex_Male','sex_Female','sex_Other','disability','age_group','worked_in_the_past'],inplace=True)\n",
    "    train_columns = X_train.columns\n",
    "    df_test = df_test.reindex(columns=train_columns)\n",
    "    # Get the prediction probabilities\n",
    "\n",
    "    final_test_pred_proba = ada_clf.predict_proba(df_test)[:, 1] \n",
    "\n",
    "    output_df = pd.DataFrame({\n",
    "        'ID': id_column,\n",
    "        'predict_proba': final_test_pred_proba\n",
    "    })\n",
    "\n",
    "    # Save the predicted probabilities to a CSV file for further analysis or submission\n",
    "    output_df.to_csv('results_9.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "pipeline()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
